{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Three: Barra's Risk Model\n",
    "\n",
    "Firstly, we define the dynamics of the price process with respect to factors(determined by step1 and step2). \n",
    "\n",
    "Secondly, using three different schemes to calibrate the coefficients.\n",
    "\n",
    "* Scheme 1: Cross-sectional regression and weighted average\n",
    "* Scheme 2: Optimization problem: minimize the exponential weighted average of squared error\n",
    "* Scheme 3: Dynamic linear model using Kalman filter\n",
    "\n",
    "## 1 Dynamics of Price\n",
    "Based on the celebrated APT model and Berra's risk model, I proposed the stock forecasting model as the following expression.\n",
    "\n",
    "$$r_{i}(t) = \\alpha(t) + \\sum_{k=1}^K \\beta_{ik}(t) F_{ik}(t)$$\n",
    "\n",
    "or more consistently,\n",
    "\n",
    "$$r_{t,i} = \\alpha_t + \\sum_{k=1}^K \\beta_{t,i,k} F_{t,i,k}$$\n",
    "\n",
    "The factor exposures $\\beta$ of the stock i to the factor k in the time t is assumed to be known in this model (a postulate), and valuated as the rescaled (normalized) factor values. \n",
    "\n",
    "For model calibration, there are three ways to handle it: a) cross-sectional regression, b) gradient descend scheme for the loss function, and c) kalman filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels import regression,stats\n",
    "import itertools\n",
    "from pykalman import KalmanFilter\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "class MultiFactorModel:\n",
    "    def __init__(self, price_df, equity_df, benchmark_df, factor_list, universe):\n",
    "        self.price_df = price_df\n",
    "        self.equity_df = equity_df\n",
    "        self.benchmark_df = benchmark_df\n",
    "        self.factor_list = factor_list\n",
    "        self.universe = universe\n",
    "        self.valid_universe = Nones\n",
    "        self.subset = None\n",
    "        \n",
    "        # z-score in universe\n",
    "        tmp_equity_df = self.calculate_subset_df(self.equity_df,self.universe)\n",
    "        tmp_equity_df = tmp_equity_df[factor_list].fillna(0) # TODO: fill or drop??\n",
    "        self.factor_zscore = (tmp_equity_df - tmp_equity_df.groupby(level='date').mean())/tmp_equity_df.groupby(level='date').std()\n",
    "        \n",
    "        # valide_universe\n",
    "        self.valid_universe = self.calculate_valid_universe()\n",
    "        self.valid_factor_zscore = self.factor_zscore.loc[idx[:,self.valid_universe],:]\n",
    "\n",
    "\n",
    "    def calculate_valid_universe(self):\n",
    "        valid_universe = set(self.universe)\n",
    "\n",
    "        for date,group in self.factor_zscore.groupby(level=0):\n",
    "            size = group.shape[0]\n",
    "            new_set = set(group.loc[idx[:,valid_universe],:].index.get_level_values(1).values)\n",
    "            if(new_set<valid_universe):\n",
    "                valid_universe = new_set\n",
    "        # self.valid_universe = valid_universe        \n",
    "        return valid_universe\n",
    "\n",
    "    def calculate_subset_df(self,df,subset):\n",
    "        return df.loc[idx[:,subset],:]\n",
    "        \n",
    "    def calculate_portfolio_risk(self,hp):\n",
    "        if(~hasattr(self,'V') or self.V==None):\n",
    "            self.cross_section_regression()\n",
    "        sigmap = np.sqrt(np.dot(np.dot(hp.T,self.V),hp))\n",
    "        MCTR = np.dot(self.V,hp)/sigmap\n",
    "        return self.V, sigmap, MCTR\n",
    "        \n",
    "    def time_series_regression(self):\n",
    "        ts_factor_zscore = sm.add_constant(self.factor_zscore)\n",
    "        Y = [self.equity_df.xs(asset,level=1)['return'] for asset in self.valid_universe]\n",
    "        X = [ts_factor_zscore.xs(asset,level=1)[factor_list+['const']] for asset in self.valid_universe]\n",
    "        reg_results = [regression.linear_model.OLS(y,x).fit().params for y,x in zip(Y,X) if not(x.empty or y.empty)]\n",
    "        indices = [asset for y, x, asset in zip(Y, X, self.valid_universe) if not(x.empty or y.empty)]\n",
    "        ts_result_df = pd.DataFrame(reg_results, index=indices)\n",
    "        return ts_result_df\n",
    "    \n",
    "#     def regression(self,tau):\n",
    "#         Xtik = self.factor_zscore.loc[idx[:,self.valid_universe],:]\n",
    "#         n_factors = len(self.factor_list)\n",
    "#         starting_point = np.zeros(n_factors)\n",
    "#         Fk = starting_point\n",
    "#         at = self.benchmark_df['return']\n",
    "#         rti = self.equity_df.loc[idx[:,self.valid_universe],'return']\n",
    "        \n",
    "#         def grad_loss(X,F,r,a,tau):\n",
    "#             n_factor = len(F)\n",
    "#             n_time = len(self.equity_df.index.get_level_values(level=0).unique())\n",
    "#             n_stock = len(self.valid_universe)\n",
    "#             grad = np.zeros(n_factor)\n",
    "#             for k in range(n_factor):\n",
    "#                 gradk = 0\n",
    "#                 for t in range(n_time):\n",
    "#                     for i in range(n_stock):\n",
    "#                         gradk += 2*tau**(n_time-t)*(a[t]+np.dot(X[t,i,:],F[:]) - r[t,i])*X[t,i,k]\n",
    "#                 grad[k] = gradk\n",
    "#             return grad\n",
    "                \n",
    "        \n",
    "    \n",
    "    \n",
    "#TODO: exponential decay regression, alpha contribution\n",
    "    def cross_section_regression(self, ndays=22, latest=True):\n",
    "        valid_factor_zscore = self.factor_zscore.loc[idx[:,self.valid_universe],:]\n",
    "        if(latest):\n",
    "            dates = valid_factor_zscore.index.get_level_values(level=0).unique()[-ndays:]\n",
    "        else:\n",
    "            dates = valid_factor_zscore.index.get_level_values(level=0).unique()[:ndays]\n",
    "        valid_factor_zscore = valid_factor_zscore.loc[idx[dates,:],:]\n",
    "        \n",
    "        result_list = []\n",
    "        date_list = []\n",
    "        for date,group in valid_factor_zscore.groupby(level=0):\n",
    "            X = sm.add_constant(group.loc[:,self.factor_list])\n",
    "            y = list(equity_df.loc[(date,list(self.valid_universe)),'return'])          \n",
    "            results = sm.regression.linear_model.OLS(y,X).fit()\n",
    "            result_list.append(results.params)\n",
    "            date_list.append(date)\n",
    "\n",
    "        cs_result_df = pd.DataFrame(result_list,index=date_list)\n",
    "        F = np.cov(cs_result_df.iloc[:,1:].T)\n",
    "        X = self.valid_factor_zscore.groupby(level=1).mean()\n",
    "        V = np.inner(np.inner(X,F),X)\n",
    "        self.F = F\n",
    "        self.X = X\n",
    "        self.V = V\n",
    "        \n",
    "        return cs_result_df\n",
    "    \n",
    "    def kalman_filter_calibration(self):\n",
    "        asset_size = len(self.valid_universe)\n",
    "        factor_size = len(self.factor_list)\n",
    "        observation_transition_matrix = []\n",
    "        for date,group in self.valid_factor_zscore.groupby(level=0):\n",
    "            exposure_matrix = group.values\n",
    "#             print(np.shape(exposure_matrix))\n",
    "#             print(asset_size)\n",
    "            observation_transition_matrix.append(\n",
    "                np.concatenate((np.eye(asset_size),exposure_matrix),axis=1).tolist()\n",
    "            )\n",
    "        observations = self.equity_df.loc[idx[:,self.valid_universe],'return']\n",
    "        observation_list = []\n",
    "        for date,group in observations.groupby(level=0):\n",
    "            observation_list.append(group.values.tolist())\n",
    "            \n",
    "        state_transition_matrix = np.eye(factor_size+asset_size)\n",
    "        state_covariance_matrix = np.eye(factor_size+asset_size)*0.01\n",
    "        observation_matrices = observation_transition_matrix\n",
    "        observation_covariance_matrix = np.eye(asset_size)*0.0\n",
    "        initial_state_mean = np.zeros(factor_size+asset_size)\n",
    "        initial_state_covariance = np.eye(factor_size+asset_size)*0.01\n",
    "\n",
    "\n",
    "        nstate = factor_size+asset_size\n",
    "        nobs = asset_size\n",
    "#         print(state_transition_matrix.shape)\n",
    "#         print(state_covariance_matrix.shape)\n",
    "#         print(np.shape(observation_matrices))\n",
    "#         print(observation_covariance_matrix.shape==(nobs,nobs))\n",
    "#         print(initial_state_mean.shape==(nstate,))\n",
    "#         print(initial_state_covariance.shape==(nstate,nstate))\n",
    "\n",
    "\n",
    "        kf = KalmanFilter(transition_matrices=state_transition_matrix,\n",
    "                 transition_covariance=state_covariance_matrix,\n",
    "                 observation_matrices=observation_matrices,\n",
    "                 observation_covariance=observation_covariance_matrix,\n",
    "                 initial_state_mean=initial_state_mean,\n",
    "                 initial_state_covariance=initial_state_covariance,\n",
    "                 n_dim_state=nstate,\n",
    "                 n_dim_obs=nobs)\n",
    "        returns = kf.filter(observation_list)\n",
    "        filtered_state_means = returns[0]\n",
    "        # a and F\n",
    "        a_list = []\n",
    "        F_list = []\n",
    "        for state in filtered_state_means:\n",
    "            a = state[:asset_size]\n",
    "            F = state[asset_size:]\n",
    "            a_list.append(a)\n",
    "            F_list.append(F)\n",
    "            \n",
    "        a_df = pd.Series(list(itertools.chain(*a_list)),index = self.valid_factor_zscore.index,name='alpha')\n",
    "        \n",
    "        F_list_temp = [[F,]*asset_size for F in F_list]\n",
    "        # subset_factor_zscore.index.get_level_values(0).unique()\n",
    "        F_list_temp = list(itertools.chain(*F_list_temp))\n",
    "        F_df = pd.DataFrame(F_list_temp,columns=[\"{}_F\".format(f) for f in factor_list],index=self.valid_factor_zscore.index)\n",
    "        \n",
    "        return_df = self.equity_df.loc[idx[:,self.valid_universe],'return']\n",
    "        close_df = self.equity_df.loc[idx[:,self.valid_universe],'close']\n",
    "        kf_df = pd.concat([self.valid_factor_zscore,F_df,a_df,return_df,close_df],axis=1)\n",
    "        self.kf_df = kf_df\n",
    "        return kf_df\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pull Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'cn_SH_healthcare_index_2012_2018.csv' does not exist: b'cn_SH_healthcare_index_2012_2018.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f3549ee1138c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprice_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minstrument_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mequity_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_price_instrument_equity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cn_stock_price_2012_2018.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"cn_instrument_info_2012_2018.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"cn_equity_daily_2012_2018.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"sectorCode\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mhealthcare_universe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstrument_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minstrument_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msectorCode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'HealthCare'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mbenchmark_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbenchmark_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cn_SH_healthcare_index_2012_2018.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mfactor_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'market_cap'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pb_ratio'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ps_ratio'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0muniverse_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstrument_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minstrument_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msectorCode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'HealthCare'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/alpha-trading/rqdata_utils.py\u001b[0m in \u001b[0;36mbenchmark_reader\u001b[0;34m(benchmark_path)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbenchmark_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbenchmark_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mbenchmark_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbenchmark_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mbenchmark_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbenchmark_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mbenchmark_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'return'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbenchmark_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbenchmark_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/finclab/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/finclab/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/finclab/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/finclab/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/finclab/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'cn_SH_healthcare_index_2012_2018.csv' does not exist: b'cn_SH_healthcare_index_2012_2018.csv'"
     ]
    }
   ],
   "source": [
    "from rqdata_utils import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import alphalens as al\n",
    "from pykalman import KalmanFilter\n",
    "\n",
    "\n",
    "price_df,instrument_df,equity_df = get_price_instrument_equity(\"cn_stock_price_2012_2018.csv\",\"cn_instrument_info_2012_2018.csv\",\"cn_equity_daily_2012_2018.csv\",\"sectorCode\")\n",
    "healthcare_universe = instrument_df.index[instrument_df.sectorCode=='HealthCare'].values\n",
    "benchmark_df = benchmark_reader(\"cn_SH_healthcare_index_2012_2018.csv\")\n",
    "factor_list = ['market_cap', 'pb_ratio', 'ps_ratio']\n",
    "universe_list = instrument_df.index[instrument_df.sectorCode=='HealthCare'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Inject Data into the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel = MultiFactorModel(price_df, equity_df, benchmark_df, factor_list, universe_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Calibration\n",
    "\n",
    "* Scheme 1: Cross-sectional regression and weighted average\n",
    "* Scheme 2: Optimization problem: minimize the exponential weighted average of squared error\n",
    "* Scheme 3: Dynamic linear model using Kalman filter\n",
    "\n",
    "The schemes one, two, three are slightly evolutional related. Scheme one is just the original BARRA's risk model defined, scheme two further defined a objective function to obtain the coefficients, and scheme three introduce Kalman filter to fit the dynamic linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Calibration Scheme 1: Cross-Sectional Regression and Weighted Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorReturn_df = myModel.cross_section_regression(ndays=132).iloc[:,(1,2,3)]\n",
    "print(factorReturn_df.head())\n",
    "hp = np.ones(len(myModel.valid_universe))\n",
    "V,sigma, MCTR = myModel.calculate_portfolio_risk(hp)\n",
    "print(\"V:\\n{}\\nsigma\\n{}\\nMCTR:\\n{}\".format(V,sigma,MCTR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_avg(array_like, tau):\n",
    "    n_element = len(array_like)\n",
    "    weights = [tau**(n_element - i-1) for i in range(n_element)]\n",
    "    return np.average(array_like,weights=weights)\n",
    "factorReturn_df.apply(lambda x: weight_avg(x,1),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this result, we can see the factor returns are under-estimated. Moreover, inconsistant with the market size effect. Here factor return for market cap should not be positive. An alternative choice would be the long-short portfolio based on each factor, which should be more practical as it can be more straight-forward to represent the factor returns. However, before doing so, let's try another optimization algorithm, gradient descend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Calibration Scheme 2: Optimization Problem and Gradient Descend\n",
    "\n",
    "The loss function:\n",
    "$$obj = \\sum_{t=1}^T \\tau^{T-t} \\sum_{i=1}^N (a_t + \\sum_{k=1}^K X_{t,i,k} f_k - r_{t,i})^2$$\n",
    "Corresponding gradient function:\n",
    "$$\\frac{\\partial obj}{\\partial f_{k^\\prime}} = \\sum_{t=1}^T \\tau^{T-t} \\sum_{i=1}^N 2 (a_t + \\sum_{k=1}^K X_{t,i,k} f_k - r_{t,i}) X_{t,i,k^{\\prime}}$$\n",
    "\n",
    "Here $\\tau$ represents the half-life period. I used a dampened sum of squared error as the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_loss(X,F,r,a,tau,n_factor,n_time,n_stock,n_days=None):\n",
    "    grad = np.zeros(n_factor)\n",
    "    for k in range(n_factor):\n",
    "        gradk = 0\n",
    "        for t in range(n_time):\n",
    "            if(n_days!=None and n_time-t>n_days):\n",
    "                continue\n",
    "            else:\n",
    "                damper = tau**(n_time-t)\n",
    "                for i in range(n_stock):\n",
    "                    gradk += 2*damper*(a[t]+np.dot(X[t,i,:],F[:]) - r[t,i])*X[t,i,k]\n",
    "        grad[k] = gradk\n",
    "    return grad\n",
    "        \n",
    "def loss(X,F,r,a,tau,n_factor,n_time,n_stock):\n",
    "    loss = 0\n",
    "    for t in range(n_time):\n",
    "        for i in range(n_stock):\n",
    "            loss += a[t] -r[t,i]\n",
    "            loss += np.dot(X[t,i,:],F[:])\n",
    "            loss = loss**2 * tau**(n_time-t)\n",
    "    return loss\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = myModel.factor_zscore.loc[idx[:,myModel.valid_universe],:]\n",
    "n_factors = len(myModel.factor_list)\n",
    "n_time = len(myModel.factor_zscore.index.get_level_values(0).unique())\n",
    "n_stocks = len(myModel.valid_universe)\n",
    "starting_point = np.zeros(n_factors)\n",
    "F = starting_point\n",
    "a_df = myModel.benchmark_df['return']\n",
    "r_df = myModel.equity_df.loc[idx[:,myModel.valid_universe],'return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_df.values.reshape(n_time,n_stocks,-1)\n",
    "a = a_df.values\n",
    "r = r_df.values.reshape(n_time,n_stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = np.zeros(n_factors)\n",
    "for step in range(100):\n",
    "    sse = loss(X[-100:,:,(0,1,2)],F,r[-100:,:],a[-100:],1,n_factors,100,n_stocks)\n",
    "    F -= grad_loss(X[-100:,:,(0,1,2)],F,r[-100:,:],a[-100:],1,n_factors,n_time=100,n_stock=n_stocks)*0.00001\n",
    "    print(sse,F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Unfortunately, the gredient descend scheme cannot guarantee to lead us to the global optimum. And it is still not helpful in interpreting market size effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Calibration Scheme 3: Kalman Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_df = myModel.kalman_filter_calibration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "kf_df['market_cap_F'].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even more unreliable. The factor return for market cap size is highly positive and volatile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "So, from all the three schemes we have discussed above, we concluded that even though BARRA's risk model has the virtue of simplicity, it is not feasible in interpreting factor returns. Or the regular calibration schemes need amendment. Personally I prefer the scheme 2 as it reserve the simplicity of the objective, but may need a better approach to get the globel optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
